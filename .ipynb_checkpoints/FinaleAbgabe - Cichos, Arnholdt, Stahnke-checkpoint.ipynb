{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import stuff & things\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convenient function to output test items\n",
    "def show_test_item(clf, i, text=True, classify=True):\n",
    "    a = 'actual: ' + str(test.iloc[i].recommended)\n",
    "    p = ''\n",
    "    if classify:\n",
    "        p = ' / predicted: ' + str(clf.predict(vectorizer.transform([test.iloc[i].content_text]))[0])\n",
    "    print('[' + str(i) + '] ' +a + p)\n",
    "    if text:\n",
    "        print('\\n')\n",
    "        print(test.iloc[i].content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_tagger(tagger, text):\n",
    "    sentences = sent_detector.tokenize(text)\n",
    "    return [tagger.tag(re.findall('(\\w+)', sentence)) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Input Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/juliusstahnke/Documents/HTW/Analytics/Analytics/Aufgabe4/'\n",
    "filename = 'classifier-based-german-pos-tagger.pickle'\n",
    "with open(path + filename, 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/german.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read Datasets for all Areas\n",
    "dir = '/Users/juliusstahnke/Documents/HTW/Analytics/Analytics/Aufgabe4'\n",
    "games = pd.read_json(dir + '/games/games.json', orient='records', encoding='utf8')\n",
    "music = pd.read_json(dir + '/music/music.json', orient='records', encoding='utf8')\n",
    "apps = pd.read_json(dir + '/apps/apps.json', orient='records', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split in Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set Randomn State\n",
    "random_state = 100\n",
    "\n",
    "#Set Traindata for games\n",
    "train_games_positiv = games[games.recommended==True].sample(frac=0.8, random_state=random_state)\n",
    "train_games_negativ = games[games.recommended==False].sample(frac=0.8, random_state=random_state)\n",
    "train_games = pd.concat([train_games_positiv, train_games_negativ])\n",
    "test_games = games.drop(train_games.index)\n",
    "\n",
    "#Set Traindata for music\n",
    "train_music_positiv = music[music.rating.isin(['8', '9', '10', '11', '12', '13', '14', '15'])].sample(frac=0.8, random_state=random_state)\n",
    "train_music_negativ = music[music.rating.isin(['1', '2', '3', '4', '5', '6', '7'])].sample(frac=0.8, random_state=random_state)\n",
    "train_music = pd.concat([train_music_positiv, train_music_negativ])\n",
    "test_music = music.drop(train_music.index)\n",
    "\n",
    "#Set Traindata for apps\n",
    "train_apps_positiv = apps[apps.Rating >= 3].sample(frac=0.8, random_state=random_state)\n",
    "train_apps_negativ = apps[apps.Rating < 3].sample(frac=0.8, random_state=random_state)\n",
    "train_apps = pd.concat([train_apps_positiv, train_apps_negativ])\n",
    "test_apps = apps.drop(train_apps.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Tableformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliusstahnke/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/juliusstahnke/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Einfügen positiv Spalte und Text\n",
    "#games.reset_index(inplace=True)\n",
    "train_games['Positiv'] = (games.recommended)\n",
    "train_games['Text'] = (games.content_text)\n",
    "\n",
    "#Einfügen positiv Spalte und Text\n",
    "#games.reset_index(inplace=True)\n",
    "train_games['Positiv'] = (games.recommended)\n",
    "train_games['Text'] = (games.content_text)\n",
    "\n",
    "#Einfügen positiv Spalte und Text\n",
    "train_apps = apps[apps.Rating!=3]\n",
    "train_apps.reset_index(inplace=True)\n",
    "train_apps['Positiv'] = (apps.Rating > 3)\n",
    "\n",
    "#Einfügen positiv Spalte und Text\n",
    "test_apps = apps[apps.Rating!=3]\n",
    "test_apps.reset_index(inplace=True)\n",
    "test_apps['Positiv'] = (apps.Rating > 3)\n",
    "\n",
    "#Einfügen positiv Spalte und Text\n",
    "train_music['Text'] = (music.description + music.summary)\n",
    "train_music.reset_index(inplace=True)\n",
    "train_music['Positiv'] = (music.rating.isin(['8', '9', '10', '11', '12', '13', '14', '15']))\n",
    "\n",
    "#Einfügen positiver Spalten und Texte\n",
    "test_music['Text'] = (music.description + music.summary)\n",
    "test_music.reset_index(inplace=True)\n",
    "test_music['Positiv'] = (music.rating.isin(['8', '9', '10', '11', '12', '13', '14', '15']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer Games untagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75987306064880111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer Games\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_games.content_text)\n",
    "\n",
    "X = vectorizer.transform(train_games.content_text)\n",
    "y = train_games.recommended\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_games.recommended\n",
    "predicted = clf.predict(vectorizer.transform(test_games.content_text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_games_tagged = [raw_tagger(tagger, a) for a in train_games.content_text]\n",
    "\n",
    "#train_games_tagged\n",
    "\n",
    "#PICKLE THE RESULT OF THIS STEP! THIS RUNS FOREVER!\n",
    "#Here is how you pickle something:\n",
    "#with open('games_pos_tagged.pickle', 'wb') as fp:\n",
    "#    pickle.dump(train_games_tagged, fp)\n",
    "    \n",
    "#If you want to load your pickled stuff use the following: \n",
    "with open('games_pos_tagged.pickle', 'rb') as fp: \n",
    "    train_games_tagged = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_games_adja = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_games_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA':\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_games_adja.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize for filtert adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55835684062059243"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer Games\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_games_adja)\n",
    "\n",
    "X = vectorizer.transform(result_games_adja)\n",
    "y = train_games.recommended\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_games.recommended\n",
    "predicted = clf.predict(vectorizer.transform(test_games.content_text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_games_multiple = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_games_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA' or word[1] == 'ADV' or word[1] == 'NN' or 'VV' in word[1]:\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_games_multiple.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71738363892806767"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer Games\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_games_multiple)\n",
    "\n",
    "X = vectorizer.transform(result_games_multiple)\n",
    "y = train_games.recommended\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_games.recommended\n",
    "predicted = clf.predict(vectorizer.transform(test_games.content_text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Apps Untagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79519799319118434"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=105, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_apps.Text)\n",
    "\n",
    "X = vectorizer.transform(train_apps.Text)\n",
    "y = train_apps.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_apps.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_apps.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_apps_tagged = [raw_tagger(tagger, a) for a in train_apps.Text]\n",
    "\n",
    "train_apps_tagged\n",
    "\n",
    "#PICKLE THE RESULT OF THIS STEP! THIS RUNS FOREVER!\n",
    "#Here is how you pickle something:\n",
    "with open('apps_pos_tagged.pickle', 'wb') as fp:\n",
    "    pickle.dump(train_apps_tagged, fp)\n",
    "    \n",
    "#If you want to load your pickled stuff use the following: \n",
    "#with open('games_pos_tagged.pickle', 'rb') as fp: \n",
    "#    test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for adjectivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_apps_adja = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_apps_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA':\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_apps_adja.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79167413247327245"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer apps\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_apps_adja)\n",
    "\n",
    "X = vectorizer.transform(result_apps_adja)\n",
    "y = train_apps.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_apps.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_apps.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_apps_multiple = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_apps_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA' or word[1] == 'ADV' or word[1] == 'NN' or 'VV' in word[1]:\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_apps_multiple.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79633279579525773"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer Apps\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_apps_multiple)\n",
    "\n",
    "X = vectorizer.transform(result_apps_multiple)\n",
    "y = train_apps.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_apps.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_apps.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Music Untagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64709705372616988"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_music.Text)\n",
    "\n",
    "X = vectorizer.transform(train_music.Text)\n",
    "y = train_music.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_music.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_music.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_music_tagged = [raw_tagger(tagger, a) for a in train_music.Text]\n",
    "\n",
    "train_music_tagged\n",
    "\n",
    "#PICKLE THE RESULT OF THIS STEP! THIS RUNS FOREVER!\n",
    "#Here is how you pickle something:\n",
    "with open('music_pos_tagged.pickle', 'wb') as fp:\n",
    "    pickle.dump(train_music_tagged, fp)\n",
    "    \n",
    "#If you want to load your pickled stuff use the following: \n",
    "#with open('games_pos_tagged.pickle', 'rb') as fp: \n",
    "#    test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Adjectiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_music_adja = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_music_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA':\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_music_adja.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Adjectiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62391681109185437"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer music\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_music_adja)\n",
    "\n",
    "X = vectorizer.transform(result_music_adja)\n",
    "y = train_music.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_music.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_music.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare result variabe\n",
    "result_music_multiple = []\n",
    "\n",
    "#For every document (Needed because Reviews Contain more than one Sentence)\n",
    "for document in train_music_tagged: \n",
    "    result_doc = \"\"\n",
    "    #Check in Every Sentence (Needed Because Senteces have more than one word)\n",
    "    for sentence in document:\n",
    "        result_sen = \"\"\n",
    "        #Check if the Second Element of the Wordtupel fullfills Condition\n",
    "        for word in sentence:\n",
    "            if word[1] == 'ADJA' or word[1] == 'ADV' or word[1] == 'NN' or 'VV' in word[1]:\n",
    "                #If so add it to Sentence, else dont\n",
    "                result_sen = result_sen + \" \" + word[0]\n",
    "        #If Sentence contains words that fullfill Condition add it to document, else dont \n",
    "        if len(result_sen) != 0:        \n",
    "            result_doc = result_doc + result_sen\n",
    "    #Add Document to Result (No Check if emty because we want reprensentation for every review in Trainingsdata)\n",
    "    result_music_multiple.append(result_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Adjectivs, adverbs, nouns and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61243500866551126"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer music\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(result_music_multiple)\n",
    "\n",
    "X = vectorizer.transform(result_music_multiple)\n",
    "y = train_music.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_music.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_music.Text))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegEx Hilfsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addNegation(str):\n",
    "    findNot = re.compile('(nicht.*?[,.!?:()])')\n",
    "    findBegining = re.compile('(^|\\s+)')\n",
    "\n",
    "    str = re.split(findNot, str)\n",
    "    i = 1\n",
    "\n",
    "    while (i < len(str)-1):\n",
    "        rstr = str[i][6:-1]\n",
    "        newstr = re.sub(findBegining, \" not_\", rstr)\n",
    "        str[i] = str[i].replace(\" \" + rstr, newstr)\n",
    "        i = i+2\n",
    "    \n",
    "    result = ''\n",
    "    for i in range(len(str)):\n",
    "        result = result + str[i]\n",
    "\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Negation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_games_negated = [addNegation(str) for str in train_games.Text]    \n",
    "test_games_negated = [addNegation(str) for str in test_games.content_text] \n",
    "\n",
    "train_apps_negated = [addNegation(str) for str in train_apps.Text]    \n",
    "test_apps_negated = [addNegation(str) for str in test_apps.Text] \n",
    "\n",
    "train_music_negated = [addNegation(str) for str in train_music.Text]    \n",
    "test_music_negated = [addNegation(str) for str in test_music.Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Negated Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76480959097320167"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer games\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_games_negated)\n",
    "\n",
    "X = vectorizer.transform(train_games_negated)\n",
    "y = train_games.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_games.recommended\n",
    "predicted = clf.predict(vectorizer.transform(test_games_negated))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Negated Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80117063847578096"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer apps\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_apps_negated)\n",
    "\n",
    "X = vectorizer.transform(train_apps_negated)\n",
    "y = train_apps.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_apps.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_apps_negated))\n",
    "accuracy_score(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize for Negated Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64774696707105717"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorizer music\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=200, token_pattern='\\w+')\n",
    "vectorizer.fit(train_music_negated)\n",
    "\n",
    "X = vectorizer.transform(train_music_negated)\n",
    "y = train_music.Positiv\n",
    "clf = MultinomialNB().fit(X, y)\n",
    "actuals = test_music.Positiv\n",
    "predicted = clf.predict(vectorizer.transform(test_music_negated))\n",
    "accuracy_score(actuals, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
